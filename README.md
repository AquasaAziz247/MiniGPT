# ğŸ¤– MiniGPT  

The simplest, cleanest repository for **building and training a GPT model from scratch** âœ¨  
This project is my hands-on practice to understand **Transformer architecture**, **tokenization**, **training loops**, and **text generation**, inspired by nanoGPT but implemented with my own learning journey in mind.  

The goal: make GPT **approachable, hackable, and fully customizable** for small- to medium-scale experiments on a single GPU or CPU âš¡  
The code is minimal, readable, and beginner-friendly â€” with `train.py` as the ~300-line training loop and `model.py` as the ~300-line Transformer definition ğŸ› ï¸  

---

## ğŸ“Œ Highlights
- ğŸ—ï¸ **From scratch** implementation of a GPT-like Transformer model  
- ğŸ”¤ **Character-level & token-level training** support  
- ğŸ“‚ **Easy dataset preparation** (Tiny Shakespeare, custom text)  
- âš™ï¸ **Configurable hyperparameters** for scaling up/down  
- âœï¸ **Sampling script** for text generation from checkpoints  
- ğŸ§© Modular, clean, and **hacker-friendly** code  

---

## ğŸ“¦ Installation  

```bash
pip install torch numpy transformers datasets tqdm
```
Dependencies ğŸ“œ

ğŸ pytorch â¤ï¸
â• numpy â¤ï¸
ğŸ¤— transformers (HuggingFace GPT-2 tools) â¤ï¸
ğŸ“Š datasets (HuggingFace dataset utilities) â¤ï¸
â³ tqdm (progress bars) â¤ï¸

ğŸš€ Quick Start
1ï¸âƒ£ Prepare your dataset
ğŸ“œ Train a character-level GPT on Shakespeare:

```bash
python data/shakespeare_char/prepare.py
```
Generates: train.bin & val.bin ğŸ“¦

2ï¸âƒ£ Train a small GPT
ğŸ’» On GPU:

```bash
python train.py config/train_shakespeare_char.py
```
ğŸ“ Context: 256
ğŸ“ Embedding: 384
ğŸ§  6 Layers Ã— 6 Heads
â±ï¸ ~3 minutes on A100 GPU

ğŸ–¥ï¸ On CPU:
```bash
python train.py config/train_shakespeare_char.py \
    --device=cpu --compile=False --block_size=64 \
    --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 \
    --max_iters=2000 --dropout=0.0
```
Runs in ~3â€“5 minutes ğŸ•’

3ï¸âƒ£ Generate text
```bash
python sample.py --out_dir=out-shakespeare-char
```
ğŸ“œ Example:
```
vbnet
```
DUKE:
I thank your eyes against it.

ANGELO:
And cowards it be strawn to my bed.
ğŸ‹ï¸â€â™‚ï¸ Reproducing GPT-2 Scale (Optional)
Train on OpenWebText:

```bash
python data/openwebtext/prepare.py
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```
ğŸ”§ Finetuning
```bash
python train.py config/finetune_shakespeare.py
```
âœ… Loads GPT-2 weights â†’ trains with small LR â†’ adapts to new data
ğŸ§ª Sampling from Pretrained Models
```bash
python sample.py \
    --init_from=gpt2-xl \
    --start="The meaning of life is" \
    --num_samples=5 --max_new_tokens=100
```
âš¡ Efficiency Tips
ğŸš€ Use torch.compile() (PyTorch 2.0) for faster training
ğŸ On Apple Silicon: --device=mps for GPU acceleration

ğŸ“… To-Do List
 ğŸ”„ Rotary Embeddings & Flash Attention
 ğŸ§® Mixed-Precision Training (fp16/bf16)
 ğŸ–¥ï¸ FSDP for large model scaling
 ğŸŒ Web-based text generation UI






